version = "3.3"


# PRIVATE

[resources]
processes = 0

[resources.cases]
input = "data/input"
output = "data/output"

[resources.conceptnet]
url = "neo4j://localhost:7687"
username = "neo4j"
password = ""

[resources.spacy]
host = "0.0.0.0"
port = 8765
workers = 1


# PUBLIC

[adaptation]
export_graph = false
export_single_stats = true
export_grid_stats = true
knowledge_graph = "wordnet"


[tuning]

adaptation_concept_candidates_limit = [5]
adaptation_method = ["direct", "bfs"] # direct, bfs
adaptation_pruning_bfs_limit = [4]
# How to prune the paths and/or lemmas of found concepts
# difference: The vector direction between the concepts should match, i.e. min cosine(abs(vec1 - vec2), abs(vec3 - vec4)).
# similarity: The absolute similarity values of the concepts should match, i.e. min abs(cosine(vec1, vec2) - cosine(vec3, vec4)).
adaptation_pruning_selector = ["similarity"]

extraction_keyword_pos_tags = [
    ["NOUN"],
] # NOUN, PROPN (not in WordNet), VERB, ADJ, ADV
extraction_keywords_per_adu = [false]
extraction_max_keywords = [10]

rules_limit = [1]

# Of which metrics should the score consist
# score_adus_sem_sim = [0, 1]
score_concept_sem_sim = [1]
score_hypernym_prox = [1]
score_keyword_weight = [1]
score_major_claim_prox = [1]
score_nodes_path_sim = [1]
# score_nodes_sem_sim = [1]
# score_nodes_wup_sim = [0, 1]
# score_query_adus_sem_sim = [0, 1]
score_query_concept_sem_sim = [1]
# score_query_nodes_sem_sim = [0, 1]

threshold_concept_score = [0.3] # Filtering concepts
threshold_nodes_similarity = [0.5] # Assigning nodes

# From what concepts should the score of adapted concepts be derived
weight_original_concept = [2]
weight_rule_source = [1]
weight_rule_target = [1]

# between: Try to transfer the concept path from (adaptation origin, adaptation destination) to (current concept, adapted concept).
# within: Try to transfer the concept path from (adaptation origin, current concept) to (adaptation destination, adapted concept).
# bfs_method = ["between"] # within CANNOT be used for generalization!


[nlp]
lang = "en"
embeddings = "integrated" # integrated, sbert, use
fuzzymax = false


[wordnet]
hypernym_filter = [
    "entity.n.01",
    "artifact.n.01",
    "causal_agent.n.01",
    "living_thing.n.01",
    "object.n.01",
    "physical_entity.n.01",
    "psychological_feature.n.01",
]
node_vector_components = ["examples", "definition"]


[conceptnet]
[conceptnet.node]
follow_form_of = false

[conceptnet.relation]
directed = true
generalization_types = [
    "DerivedFrom",
    "IsA",
    "PartOf",
    # "HasA",
]
all_types = [
    "Antonym",
    "AtLocation",
    "CapableOf",
    "Causes",
    "CausesDesire",
    "CreatedBy",
    "DefinedAs",
    "DerivedFrom",
    "Desires",
    "DistinctFrom",
    "Entails",
    "EtymologicallyDerivedFrom",
    "EtymologicallyRelatedTo",
    "ExternalURL",
    "FormOf",
    "HasA",
    "HasContext",
    "HasFirstSubevent",
    "HasLastSubevent",
    "HasPrerequisite",
    "HasProperty",
    "HasSubevent",
    "IsA",
    "IsInstanceOf",
    "LocatedNear",
    "MadeOf",
    "MannerOf",
    "MotivatedByGoal",
    "NotCapableOf",
    "NotDesires",
    "NotHasProperty",
    "ObstructedBy",
    "PartOf",
    "ReceivesAction",
    "RelatedTo",
    "SimilarTo",
    "SymbolOf",
    "Synonym",
    "UsedFor",
]

[conceptnet.path]
[conceptnet.path.max_length]
shortest_path = 15
generalization = 15
metrics = 15
