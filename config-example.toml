version = "3.3"


# PRIVATE

[resources]
processes = 0

[resources.cases]
input = "data/input"
output = "data/output"

[resources.conceptnet]
url = "neo4j://localhost:7687"
username = "neo4j"
password = ""

[resources.spacy]
host = "0.0.0.0"
port = 8765
workers = 1


# PUBLIC

[loading]
min_synset_similarity = 0.5

[adaptation]
export_graph = false
export_single_stats = true
export_grid_stats = true
knowledge_graph = "wordnet"
bfs_node_limit = 4
rules_limit = 1


[tuning]
# [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
extraction_keyword_pos_tags = [
    ["NOUN"],
    ["NOUN", "VERB", "ADJ"],
] # NOUN, PROPN (not in wordnet), VERB, ADJ, ADV
extraction_max_keywords = [5, 10]
extraction_keywords_per_adu = [false]

# Threshold for filtering concepts
threshold_concept_score = [0.0, 0.25, 0.5]

# Threshold for assigning nodes to a concept based on their definition/examples
threshold_synset_similarity = [0.0, 0.25, 0.5]

# Of which metrics should the score consist
score_adus_semantic_similarity = [0, 1]
score_concept_semantic_similarity = [0, 1]
score_hypernym_proximity = [0, 1]
score_keyword_weight = [0, 1]
score_major_claim_proximity = [0, 1]
score_nodes_path_similarity = [0, 1]
score_nodes_semantic_similarity = [1]
score_nodes_wup_similarity = [0, 1]
score_query_adus_semantic_similarity = [0, 1]
score_query_concept_semantic_similarity = [0, 1]
score_query_nodes_semantic_similarity = [0, 1]

# From what concepts should the metrics be derived
weight_original_concept = [3]
weight_rule_source = [1]
weight_rule_target = [1]

# How to perform the adaptation: direct, bfs
adaptation_method = ["direct"]

# between: Try to transfer the concept path from (adaptation origin, adaptation destination) to (current concept, adapted concept).
# within: Try to transfer the concept path from (adaptation origin, current concept) to (adaptation destination, adapted concept).
bfs_method = ["between"] # within CANNOT be used for generalization!

# difference: The vector direction between the concepts should match, i.e. min cosine(abs(vec1 - vec2), abs(vec3 - vec4)).
# similarity: The absolute similarity values of the concepts should match, i.e. min abs(cosine(vec1, vec2) - cosine(vec3, vec4)).
bfs_selector = ["similarity"]


[nlp]
max_distance = 50
lang = "en"
embeddings = "integrated" # integrated, sbert, use
fuzzymax = false


[wordnet]
hypernym_filter = [
    "entity.n.01",
    "artifact.n.01",
    "causal_agent.n.01",
    "living_thing.n.01",
    "object.n.01",
    "physical_entity.n.01",
    "psychological_feature.n.01",
]
node_vector_components = ["examples", "definition"]


[conceptnet]
[conceptnet.node]
follow_form_of = false

[conceptnet.relation]
directed = true
generalization_types = [
    "DerivedFrom",
    "IsA",
    "PartOf",
    # "HasA",
]
all_types = [
    "Antonym",
    "AtLocation",
    "CapableOf",
    "Causes",
    "CausesDesire",
    "CreatedBy",
    "DefinedAs",
    "DerivedFrom",
    "Desires",
    "DistinctFrom",
    "Entails",
    "EtymologicallyDerivedFrom",
    "EtymologicallyRelatedTo",
    "ExternalURL",
    "FormOf",
    "HasA",
    "HasContext",
    "HasFirstSubevent",
    "HasLastSubevent",
    "HasPrerequisite",
    "HasProperty",
    "HasSubevent",
    "IsA",
    "IsInstanceOf",
    "LocatedNear",
    "MadeOf",
    "MannerOf",
    "MotivatedByGoal",
    "NotCapableOf",
    "NotDesires",
    "NotHasProperty",
    "ObstructedBy",
    "PartOf",
    "ReceivesAction",
    "RelatedTo",
    "SimilarTo",
    "SymbolOf",
    "Synonym",
    "UsedFor",
]

[conceptnet.path]
[conceptnet.path.max_length]
shortest_path = 10
generalization = 3
